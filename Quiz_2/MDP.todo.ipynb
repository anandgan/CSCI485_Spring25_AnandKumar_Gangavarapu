{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf8a3c62-216c-446d-851d-7249cc67787a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "table {align:left;display:block} \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {align:left;display:block} \n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e27c8c-7f53-4e9f-b4da-7e0343fb6313",
   "metadata": {},
   "source": [
    "# Markov Decision Process (MDP)\n",
    "----\n",
    "\n",
    "**Value Iteration Process with Policy Changes in MDP**\n",
    "\n",
    "We begin with a Markov Decision Process (MDP) where an agent decides whether to invest conservatively (C) or aggressively (A) in a financial portfolio. The objective is to find an optimal policy maximizing long-term rewards.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Defining the MDP Components**\n",
    "\n",
    "**States (S):**\n",
    "\n",
    "- Low Wealth (L)\n",
    "- Medium Wealth (M)\n",
    "- High Wealth (H)\n",
    "\n",
    "**Actions (A):**\n",
    "\n",
    "- Conservative (C)\n",
    "- Aggressive (A)\n",
    "\n",
    "**Transition Probabilities:**\n",
    "\n",
    ">| Current State | Action | Next State Probabilities     |\n",
    "| ------------- | ------ | ---------------------------- |\n",
    "| Low (L)       | C      | 80% Stay in L, 20% Move to M |\n",
    "| Low (L)       | A      | 60% Stay in L, 40% Move to M |\n",
    "| Medium (M)    | C      | 70% Stay in M, 30% Move to H |\n",
    "| Medium (M)    | A      | 50% Stay in M, 50% Move to H |\n",
    "| High (H)      | C      | 90% Stay in H, 10% Drop to M |\n",
    "| High (H)      | A      | 70% Stay in H, 30% Drop to M |\n",
    "\n",
    "**Rewards:**\n",
    "\n",
    "- Low Wealth (L): -1\n",
    "- Medium Wealth (M): 3\n",
    "- High Wealth (H): 5\n",
    "\n",
    "**Discount Factor (γ):** 0.9\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6becd-538b-4010-b4ed-c2e03c6c2e1a",
   "metadata": {},
   "source": [
    "### **Step 2: Value Iteration Updates**\n",
    "\n",
    "We initialize values: $V_0(L) = 0$, $V_0(M) = 0$, $V_0(H) = 0$.\n",
    "\n",
    "#### **Iteration 1**\n",
    "\n",
    "Using Bellman’s equation:\n",
    "\n",
    ">$\n",
    "V_1(s) = \\max_a \\left[ R(s) + \\gamma \\sum_{s'} P(s' | s, a) V_0(s') \\right]\n",
    "$\n",
    "\n",
    "For **Low Wealth (L):**\n",
    "\n",
    ">$\n",
    "V_1(L) = \\max \\left[ -1 + 0.9(0.8V_0(L) + 0.2V_0(M)), -1 + 0.9(0.6V_0(L) + 0.4V_0(M)) \\right]\n",
    "$\n",
    "\n",
    "For **Medium Wealth (M):**\n",
    "\n",
    ">$\n",
    "V_1(M) = \\max \\left[ 3 + 0.9(0.7V_0(M) + 0.3V_0(H)), 3 + 0.9(0.5V_0(M) + 0.5V_0(H)) \\right]\n",
    "$\n",
    "\n",
    "For **High Wealth (H):**\n",
    "\n",
    ">$\n",
    "V_1(H) = \\max \\left[ 5 + 0.9(0.9V_0(H) + 0.1V_0(M)), 5 + 0.9(0.7V_0(H) + 0.3V_0(M)) \\right]\n",
    "$\n",
    "\n",
    "Since $V_0(L) = V_0(M) = V_0(H) = 0$, the initial values are just the rewards.\n",
    "\n",
    ">$\n",
    "V_1(L) = -1, \\quad V_1(M) = 3, \\quad V_1(H) = 5\n",
    "$\n",
    "\n",
    "#### **Policy Evaluation after Iteration 1**\n",
    "\n",
    "> \\$\n",
    "Q(L, C) = -1 + 0.9(0.8(-1) + 0.2(3)) = -1.44 (should be -1.18)\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(L, A) = -1 + 0.9(0.6(-1) + 0.4(3)) = -0.76 (should be -0.46)\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(M, C) = 3 + 0.9(0.7(3) + 0.3(5)) = 6.24\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(M, A) = 3 + 0.9(0.5(3) + 0.5(5)) = 6.6\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(H, C) = 5 + 0.9(0.9(5) + 0.1(3)) = 9.32\n",
    "\\$\n",
    "\n",
    "> \\$\n",
    "Q(H, A) = 5 + 0.9(0.7(5) + 0.3(3)) = 8.96\n",
    "\\$\n",
    "\n",
    "\n",
    "**Policy at Iteration 1:**\n",
    "- L → Aggressive(A)\n",
    "- M → Aggressive (A)\n",
    "- H → Conservative (C)\n",
    "\n",
    "\n",
    "#### **Iteration 2**\n",
    "\n",
    "Updating $V_2(s)$:\n",
    "\n",
    ">$\n",
    "V_2(L) = \\max \\left[ -1 + 0.9(0.8(-1) + 0.2(3)), -1 + 0.9(0.6(-1) + 0.4(3)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_2(M) = \\max \\left[ 3 + 0.9(0.7(3) + 0.3(5)), 3 + 0.9(0.5(3) + 0.5(5)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_2(H) = \\max \\left[ 5 + 0.9(0.9(5) + 0.1(3)), 5 + 0.9(0.7(5) + 0.3(3)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_2(L) = -0.46, \\quad V_2(M) = 6.6, \\quad V_2(H) = 9.32\n",
    "$\n",
    "\n",
    "#### **Policy Evaluation after Iteration 2**\n",
    "\n",
    ">$ \n",
    "Q(L, C) = -1 + 0.9(0.8(-0.46) + 0.2(6.6)) = -0.1432 \n",
    "\n",
    " Q(L, A) = -1 + 0.9(0.6(-0.46) + 0.4(6.6)) = 1.1276 \n",
    "\n",
    " $ the max is Q(L,A)$\n",
    " Q(M, C) = 3 + 0.9(0.7(6.6) + 0.3(9.32)) = 9.6744 \n",
    "\n",
    " Q(M, A) = 3 + 0.9(0.5(6.6) + 0.5(9.32)) = 10.164 \n",
    " >$ The max is Q(M,A)\n",
    "\n",
    " Q(H, C) = 5 + 0.9(0.9(9.32) + 0.1(6.6)) = 13.1432 \n",
    "\n",
    " Q(H, A) = 5 + 0.9(0.7(9.32) + 0.3(6.6)) = 12.6536 $\n",
    " $ The max is Q(H,C) \n",
    "\n",
    "**Policy at Iteration 2:**\n",
    "- L → Aggressive\n",
    "- M → Aggressive \n",
    "- H → Conservative\n",
    "\n",
    "#### **Iteration 3**\n",
    "\n",
    "Updating $V_3(s)$:\n",
    "\n",
    ">$\n",
    "V_3(L) = \\max \\left[ -1 + 0.9(0.8V_2(L) + 0.2V_2(M)), -1 + 0.9(0.6V_2(L) + 0.4V_2(M)) \\right]\n",
    "\n",
    "V_3(L) = \\max \\left[ -1 + 0.9(0.8(-0.46) + 0.2(6.6)), -1 + 0.9(0.6(-0.46) + 0.4(6.6)) \\right]\n",
    "$\n",
    "\n",
    ">$ max(-0.1432,1.1276) = 1.1276$\n",
    "\n",
    ">$\n",
    "V_3(M) = \\max \\left[ 3 + 0.9(0.7V_2(M) + 0.3V_2(H)), 3 + 0.9(0.5V_2(M) + 0.5V_2(H)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_3(M) = \\max \\left[ 3 + 0.9(0.7(6.6) + 0.3(9.32)), 3 + 0.9(0.5(6.6) + 0.5(9.32)) \\right]\n",
    "$\n",
    ">$ max(9.6744,10.164) = 10.164$\n",
    "\n",
    ">$\n",
    "V_3(H) = \\max \\left[ 5 + 0.9(0.9V_2(H) + 0.1V_2(M)), 5 + 0.9(0.7V_2(H) + 0.3V_2(M)) \\right]\n",
    "$\n",
    "\n",
    ">$\n",
    "V_3(H) = \\max \\left[ 5 + 0.9(0.9(9.32) + 0.1(6.6)), 5 + 0.9(0.7(9.32) + 0.3(6.6)) \\right]\n",
    "$\n",
    ">$ max(13.1432,12.6536) = 13.1432$\n",
    "\n",
    "\n",
    "\n",
    "#### **Policy Change Analysis**\n",
    "\n",
    "From **Iteration 2 to Iteration 3**, let’s check the action values to determine if the policy changed.\n",
    "\n",
    "For **Low Wealth (L):**\n",
    "\n",
    ">$Q(L, C) = -1 + 0.9(0.8V_3(L)+0.2V_3(M))\n",
    "$\n",
    "\n",
    ">$Q(L, C) = -1 + 0.9(0.8(1.1276)+0.2(10.164))\n",
    "\n",
    ">$\n",
    "Q(L, A) = -1 + 0.9(0.6V_3(L)+0.4V_3(M))$\n",
    "Q(L, A) = -1 + 0.9(0.6(1.1276)+0.4(10.164))$\n",
    "\n",
    ">$ the values we get (1.6414 , 3.2679)\n",
    "\n",
    ">$ The max is Q(L,A) \n",
    "\n",
    "For **Medium Wealth (M):**\n",
    "\n",
    ">$\n",
    "Q(M, C) = 3+ 0.9(0.7V_3(M)+0.3V_3(H))\n",
    "Q(M, C) = 3+ 0.9(0.7(10.164)+0.3(13.1432))\n",
    "$\n",
    "\n",
    ">$\n",
    "Q(M, A) = 3 + 0.9(0.5V_3(M)+0.5V_3(H))\n",
    "Q(M, A) = 3 + 0.9(0.5(10.164)+0.5(13.1432))\n",
    "$\n",
    "\n",
    ">$ the values we get is (12.952 , 13.4882)\n",
    "\n",
    ">$ the max is Q(M,A)$\n",
    "\n",
    "For **High Wealth (H):**\n",
    "\n",
    ">$\n",
    "Q(H, C) = 5 + 0.9(0.9V_3(H)+0.1V_3(M))\n",
    "Q(H, C) = 5 + 0.9(0.9(13.1432)+0.1(10.164))\n",
    "\n",
    "$\n",
    "\n",
    ">$\n",
    "Q(H, A) = 5 + 0.9(0.7V_3(H)+0.3V_3(M))\n",
    "Q(H, A) = 5 + 0.9(0.7(13.1432)+0.3(10.164))\n",
    "$\n",
    "\n",
    ">$ the values we get is (16.5607,16.0245)\n",
    "\n",
    ">$ the max is Q(H,C) \n",
    "\n",
    "Compare $Q(L, A), Q(L, C)$ and $Q(H, C),  Q(H, A)$, decide the policy updates:\n",
    "\n",
    "- **Low Wealth (L)** → Aggressive\n",
    "- **Medium Wealth (M)** → Aggressive \n",
    "- **High Wealth (H)** → Conservative\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e5695-a01b-4bcc-909b-582b5e64d068",
   "metadata": {},
   "source": [
    "### Summary: Policy Evolution Over Iterations\n",
    "\n",
    ">| State  | Iteration 1 | Iteration 2 | Iteration 3 |\n",
    "|--------|------------|------------|------------|\n",
    "| Low      | A        | A          | A          |\n",
    "| Medium   | A        | A          | A          |\n",
    "| High     | C        | C          | C          |\n",
    "\n",
    "TODO: Analysis: we have computed three policy evolution over three iterations \n",
    "the policy we deduced is low income sector and medium sector should be aggressive regarding their investment but only high income income sector should be conservative \n",
    "\n",
    "so even if go under another iteration and get policy the same results will come\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
